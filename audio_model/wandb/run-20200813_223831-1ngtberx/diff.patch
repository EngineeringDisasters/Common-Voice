diff --git a/audio_model/VERSION.txt b/audio_model/VERSION.txt
index 8a9ecc2..7bcd0e3 100644
--- a/audio_model/VERSION.txt
+++ b/audio_model/VERSION.txt
@@ -1 +1 @@
-0.0.1
\ No newline at end of file
+0.0.2
\ No newline at end of file
diff --git a/audio_model/config/config.py b/audio_model/config/config.py
index 8c30fec..436a2f5 100644
--- a/audio_model/config/config.py
+++ b/audio_model/config/config.py
@@ -41,8 +41,8 @@ class CommonVoiceModels:
                   4: 'American'}
 
         NAME = "model_country-"
-        PARAM = {'HIDDEN_DIM': 256, 'NUM_LAYERS': 2, 'DROPOUT': 0.15, 'INPUT_SIZE': 13, 'BATCH_SIZE': 8,
-                 'OUTPUT_SIZE': 5, 'LEARNING_RATE': 0.0001, 'GRADIENT_CLIP': 35, 'EPOCH': 1}
+        PARAM = {'HIDDEN_DIM': 8, 'NUM_LAYERS': 2, 'DROPOUT': 0.15, 'INPUT_SIZE': 13, 'BATCH_SIZE': 512,
+                 'OUTPUT_SIZE': 5, 'LEARNING_RATE': 0.0001, 'GRADIENT_CLIP': 35, 'EPOCH': 2}
         LABEL = 'accent'
 
 
diff --git a/audio_model/model_manager.py b/audio_model/model_manager.py
index 31420af..fe29d01 100644
--- a/audio_model/model_manager.py
+++ b/audio_model/model_manager.py
@@ -5,7 +5,6 @@ import numpy as np
 import torch
 import torch.nn as nn
 import wandb
-
 from utlis import _metric_summary, log_scalar
 
 warnings.filterwarnings("ignore")
@@ -76,7 +75,7 @@ def train(
         learning_rate,
         train_loader: torch.utils.data.dataloader.DataLoader,
         valid_loader: torch.utils.data.dataloader.DataLoader,
-        print_every: int = 10,
+        print_every: int = 1,
         early_stopping_threshold: int = 20,
         early_stopping: bool = True,
 ) -> object:
@@ -107,6 +106,8 @@ def train(
         model.cuda()
 
     counter = 0
+    running_loss_train= []
+    running_loss_val = []
 
     for e in range(epoch):
         for train_inputs, train_labels in train_loader:
@@ -133,46 +134,48 @@ def train(
             log_scalar(name="Precision/train", value=train_rc, step=counter)
             log_scalar(name="Recall/train", value=train_rc, step=counter)
             log_scalar(name="Loss/train", value=train_loss.item(), step=counter)
+            running_loss_train.append(train_loss.item())
 
             if counter % print_every == 0:
 
                 model.init_hidden()
                 model.eval()
 
-                for val_inputs, val_labels in valid_loader:
-
-                    if torch.cuda.is_available():
-                        val_inputs, val_labels = val_inputs.cuda(), val_labels.cuda()
-
-                    val_output = model(val_inputs)
-                    val_loss = criterion(val_output, val_labels)
-
-                    val_acc, val_pr, val_rc = _metric_summary(
-                        pred=torch.max(val_output, dim=1).indices.data.cpu().numpy(),
-                        label=val_labels.cpu().numpy(),
-                    )
-
-                    wandb.log({"Accuracy/val": val_acc}, step=counter)
-                    wandb.log({"Precision/val": val_pr}, step=counter)
-                    wandb.log({"Recall/val": val_rc}, step=counter)
-                    wandb.log({"Loss/val": val_loss.item()}, step=counter)
-
-                model.train()
-                _logger.info(
-                    "Epoch: {}/{}...Step: {}..."
-                    "Training Loss: {:.3f}..."
-                    "Validation Loss: {:.3f}..."
-                    "Train Accuracy: {:.3f}..."
-                    "Test Accuracy: {:.3f}".format(
-                        e + 1,
-                        epoch,
-                        counter,
-                        train_loss.item(),
-                        val_loss.item(),
-                        train_acc,
-                        val_acc,
-                    )
+        for val_inputs, val_labels in valid_loader:
+
+            if torch.cuda.is_available():
+                val_inputs, val_labels = val_inputs.cuda(), val_labels.cuda()
+
+            val_output = model(val_inputs)
+            val_loss = criterion(val_output, val_labels)
+            running_loss_val.append(val_loss.item())
+
+            val_acc, val_pr, val_rc = _metric_summary(
+                pred=torch.max(val_output, dim=1).indices.data.cpu().numpy(),
+                label=val_labels.cpu().numpy(),
+            )
+
+            wandb.log({"Accuracy/val": val_acc}, step=counter)
+            wandb.log({"Precision/val": val_pr}, step=counter)
+            wandb.log({"Recall/val": val_rc}, step=counter)
+            wandb.log({"Loss/val": val_loss.item()}, step=counter)
+
+            model.train()
+            _logger.info(
+                "Epoch: {}/{}...Step: {}..."
+                "Training Loss: {:.3f}..."
+                "Validation Loss: {:.3f}..."
+                "Train Accuracy: {:.3f}..."
+                "Test Accuracy: {:.3f}".format(
+                    e + 1,
+                    epoch,
+                    counter,
+                    np.mean(running_loss_train),
+                    np.mean(running_loss_val),
+                    train_acc,
+                    val_acc,
                 )
+            )
 
         if early_stopping:
             stopping(val_loss=val_loss, model=model)
@@ -180,11 +183,11 @@ def train(
                 _logger.info("Stopping Model Early")
                 break
 
-    # wandb.sklearn.plot_confusion_matrix(
-    #     val_labels.cpu().numpy(),
-    #     torch.max(val_output, dim=1).indices.data.cpu().numpy(),
-    #     valid_loader.dataset.classes,
-    # )
+    wandb.sklearn.plot_confusion_matrix(
+        val_labels.cpu().numpy(),
+        torch.max(val_output, dim=1).indices.data.cpu().numpy(),
+        valid_loader.dataset.classes,
+    )
 
     _logger.info("Done Training, uploaded model to {}".format(wandb.run.dir))
     return model
